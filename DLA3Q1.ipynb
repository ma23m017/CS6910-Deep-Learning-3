{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kcZlXza40edm"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "class MyEncoder(nn.Module):\n",
        "    def __init__(self, input_size=72, embedding_size=64, hidden_size=256, cell_type='gru', num_layers=2, bidirectional=True, dropout_prob=0,\n",
        "                 device=torch.device('cuda' if torch.cuda.is_available() else 'cpu')):\n",
        "        super(MyEncoder, self).__init__()\n",
        "\n",
        "        # Configuration parameters for the encoder\n",
        "        self.config = {\n",
        "            'input_size': input_size,\n",
        "            'embedding_size': embedding_size,\n",
        "            'hidden_size': hidden_size,\n",
        "            'cell_type': cell_type,\n",
        "            'num_layers': num_layers,\n",
        "            'bidirectional': bidirectional,\n",
        "            'dropout_prob': dropout_prob,\n",
        "            'device': device.type\n",
        "        }\n",
        "\n",
        "        # Assigning parameters to instance variables\n",
        "        self.input_size = input_size\n",
        "        self.embedding_size = embedding_size\n",
        "        self.hidden_size = hidden_size\n",
        "        self.cell_type = cell_type\n",
        "        self.num_layers = num_layers\n",
        "        self.bidirectional = bidirectional\n",
        "        self.dropout_prob = dropout_prob\n",
        "        self.device = device\n",
        "\n",
        "        # Embedding layer\n",
        "        self.embedding_layer = nn.Embedding(input_size, embedding_size)\n",
        "        # Dropout layer\n",
        "        self.dropout_layer = nn.Dropout(dropout_prob)\n",
        "\n",
        "        # Calculating the number of directions based on bidirectionality\n",
        "        self.directions = 2 if bidirectional else 1\n",
        "\n",
        "        # Instantiating the appropriate RNN layer based on cell type\n",
        "        if cell_type == 'rnn':\n",
        "            self.rnn_layer = MyRNNLayer(embedding_size, hidden_size, num_layers, bidirectional, dropout_prob)\n",
        "        elif cell_type == 'gru':\n",
        "            self.rnn_layer = MyGRULayer(embedding_size, hidden_size, num_layers, bidirectional, dropout_prob)\n",
        "        elif cell_type == 'lstm':\n",
        "            self.rnn_layer = MyLSTMLayer(embedding_size, hidden_size, num_layers, bidirectional, dropout_prob)\n",
        "\n",
        "    def forward(self, input_seq, hidden_state, cell_state=None):\n",
        "        # Embedding the input sequence\n",
        "        embedded_seq = self.embedding_layer(input_seq)\n",
        "        # Applying dropout to the embedded sequence\n",
        "        embedded_seq = self.dropout_layer(embedded_seq)\n",
        "\n",
        "        # Forward pass through the RNN layer\n",
        "        if self.cell_type == 'lstm':\n",
        "            output_seq, (hidden_state, cell_state) = self.rnn_layer(embedded_seq, (hidden_state, cell_state))\n",
        "        else:\n",
        "            output_seq, hidden_state = self.rnn_layer(embedded_seq, hidden_state)\n",
        "\n",
        "        # Returning the output sequence and hidden/cell states (if LSTM)\n",
        "        return output_seq, hidden_state, cell_state if self.cell_type == 'lstm' else None\n",
        "\n",
        "    def get_config(self):\n",
        "        # Method to retrieve the configuration parameters\n",
        "        return self.config\n",
        "\n",
        "    def init_hidden_state(self, batch_size):\n",
        "        # Method to initialize the hidden state\n",
        "        return torch.zeros(self.directions * self.num_layers, batch_size, self.hidden_size, device=self.device)\n",
        "\n",
        "class MyRNNLayer(nn.Module):\n",
        "    def __init__(self, input_size, hidden_size, num_layers, bidirectional, dropout_prob):\n",
        "        super(MyRNNLayer, self).__init__()\n",
        "        # RNN layer instantiation\n",
        "        self.rnn = nn.RNN(input_size=input_size, hidden_size=hidden_size, num_layers=num_layers, bidirectional=bidirectional, dropout=dropout_prob)\n",
        "\n",
        "    def forward(self, input_seq, hidden_state):\n",
        "        # Forward pass through the RNN layer\n",
        "        return self.rnn(input_seq, hidden_state)\n",
        "\n",
        "class MyGRULayer(nn.Module):\n",
        "    def __init__(self, input_size, hidden_size, num_layers, bidirectional, dropout_prob):\n",
        "        super(MyGRULayer, self).__init__()\n",
        "        # GRU layer instantiation\n",
        "        self.gru = nn.GRU(input_size=input_size, hidden_size=hidden_size, num_layers=num_layers, bidirectional=bidirectional, dropout=dropout_prob)\n",
        "\n",
        "    def forward(self, input_seq, hidden_state):\n",
        "        # Forward pass through the GRU layer\n",
        "        return self.gru(input_seq, hidden_state)\n",
        "\n",
        "class MyLSTMLayer(nn.Module):\n",
        "    def __init__(self, input_size, hidden_size, num_layers, bidirectional, dropout_prob):\n",
        "        super(MyLSTMLayer, self).__init__()\n",
        "        # LSTM layer instantiation\n",
        "        self.lstm = nn.LSTM(input_size=input_size, hidden_size=hidden_size, num_layers=num_layers, bidirectional=bidirectional, dropout=dropout_prob)\n",
        "\n",
        "    def forward(self, input_seq, hidden_state):\n",
        "        # Forward pass through the LSTM layer\n",
        "        return self.lstm(input_seq, hidden_state)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "class MyDecoder(nn.Module):\n",
        "    def __init__(self, input_size=26, embedding_size=64, hidden_size=256, cell_type='lstm', num_layers=2, use_attention=False,\n",
        "                 attention_size=None, dropout=0, bidirectional=True,\n",
        "                 device=torch.device('cuda' if torch.cuda.is_available() else 'cpu')):\n",
        "        super(MyDecoder, self).__init__()\n",
        "\n",
        "        # Decoder parameters\n",
        "        self.input_size = input_size  # Number of unique input symbols\n",
        "        self.embedding_size = embedding_size  # Dimensionality of the embedding space\n",
        "        self.hidden_size = hidden_size  # Dimensionality of the hidden state\n",
        "        self.cell_type = cell_type  # Type of recurrent cell (RNN, GRU, LSTM)\n",
        "        self.num_layers = num_layers  # Number of recurrent layers\n",
        "        self.use_attention = use_attention  # Flag indicating whether to use attention mechanism\n",
        "        self.attention_size = attention_size  # Dimensionality of attention mechanism\n",
        "        self.dropout = dropout  # Dropout probability\n",
        "        self.bidirectional = bidirectional  # Flag indicating bidirectional RNN\n",
        "        self.device = device  # Device to run computations\n",
        "\n",
        "        # Components initialization\n",
        "        self._build_embedding_layer()  # Initialize embedding layer\n",
        "        self._build_dropout_layer()  # Initialize dropout layer\n",
        "        self._build_decoder_rnn()  # Initialize decoder RNN\n",
        "        if use_attention:\n",
        "            self._build_attention_mechanism()  # Initialize attention mechanism components\n",
        "        self._build_output_layer()  # Initialize output layer\n",
        "\n",
        "    def forward(self, input_seq, hidden_state, cell_state=None, encoder_outputs=None):\n",
        "        # Embed input sequence\n",
        "        embedded_seq = self.embedding(input_seq)\n",
        "        embedded_seq = self.dropout(embedded_seq)\n",
        "\n",
        "        # Apply attention mechanism if enabled\n",
        "        if self.use_attention:\n",
        "            context, attention_weights = self._apply_attention(hidden_state, encoder_outputs)\n",
        "            embedded_seq = torch.cat((embedded_seq, context), 2)\n",
        "\n",
        "        # Pass through decoder RNN\n",
        "        output, hidden_state, cell_state = self.decoder_rnn(embedded_seq, (hidden_state, cell_state) if self.cell_type == 'lstm' else hidden_state)\n",
        "\n",
        "        # Apply output layer to match output dimension\n",
        "        output = self.output_layer(output)\n",
        "\n",
        "        return output, hidden_state, cell_state, attention_weights if self.use_attention else None\n",
        "\n",
        "    # Method to build embedding layer\n",
        "    def _build_embedding_layer(self):\n",
        "        self.embedding = nn.Embedding(self.input_size, self.embedding_size)\n",
        "\n",
        "    # Method to build dropout layer\n",
        "    def _build_dropout_layer(self):\n",
        "        self.dropout = nn.Dropout(self.dropout)\n",
        "\n",
        "    # Method to build decoder RNN\n",
        "    def _build_decoder_rnn(self):\n",
        "        input_size = self.embedding_size + (self.attention_size if self.use_attention else 0)\n",
        "        if self.cell_type == 'lstm':\n",
        "            self.decoder_rnn = nn.LSTM(input_size=input_size, hidden_size=self.hidden_size, num_layers=self.num_layers,\n",
        "                                       bidirectional=self.bidirectional, dropout=self.dropout)\n",
        "        elif self.cell_type == 'gru':\n",
        "            self.decoder_rnn = nn.GRU(input_size=input_size, hidden_size=self.hidden_size, num_layers=self.num_layers,\n",
        "                                      bidirectional=self.bidirectional, dropout=self.dropout)\n",
        "        else:\n",
        "            self.decoder_rnn = nn.RNN(input_size=input_size, hidden_size=self.hidden_size, num_layers=self.num_layers,\n",
        "                                      bidirectional=self.bidirectional, dropout=self.dropout)\n",
        "\n",
        "    # Method to build attention mechanism\n",
        "    def _build_attention_mechanism(self):\n",
        "        self.attention_W = nn.Linear(self.hidden_size, self.hidden_size)\n",
        "        self.attention_U = nn.Linear(self.hidden_size, self.hidden_size)\n",
        "        self.attention_V = nn.Linear(self.hidden_size, 1)\n",
        "\n",
        "    # Method to build output layer\n",
        "    def _build_output_layer(self):\n",
        "        output_size = self.input_size\n",
        "        if self.bidirectional:\n",
        "            output_size *= 2\n",
        "        self.output_layer = nn.Linear(self.hidden_size, output_size)\n",
        "\n",
        "    # Method to apply attention mechanism\n",
        "    def _apply_attention(self, hidden_state, encoder_outputs):\n",
        "        encoder_transform = self.attention_W(encoder_outputs)\n",
        "        hidden_transform = self.attention_U(hidden_state)\n",
        "        concat_transform = encoder_transform + hidden_transform\n",
        "        concat_transform = torch.tanh(concat_transform)\n",
        "        score = self.attention_V(concat_transform)\n",
        "        attention_weights = F.softmax(score, dim=1)\n",
        "        context_vector = torch.sum(attention_weights * encoder_outputs, dim=1)\n",
        "        normalized_context_vector = context_vector.unsqueeze(0)\n",
        "\n",
        "        return normalized_context_vector, attention_weights\n",
        "\n",
        "    # Method to get decoder parameters\n",
        "    def get_params(self):\n",
        "        return {\n",
        "            'input_size': self.input_size,\n",
        "            'embedding_size': self.embedding_size,\n",
        "            'hidden_size': self.hidden_size,\n",
        "            'attention_size': self.attention_size,\n",
        "            'cell_type': self.cell_type,\n",
        "            'num_layers': self.num_layers,\n",
        "            'dropout': self.dropout.p,\n",
        "            'bidirectional': self.bidirectional,\n",
        "            'device': self.device.type,\n",
        "            'use_attention': self.use_attention,\n",
        "            'attention_size': self.attention_size\n",
        "        }\n",
        "\n",
        "# note that this only functional part of code and not the whole one, so it will throw error on running this code\n"
      ],
      "metadata": {
        "id": "LHef3JkCRCbp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "XXt6lt6ZUZUr"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}