{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":8212755,"sourceType":"datasetVersion","datasetId":4867367}],"dockerImageVersionId":30699,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport torch.optim as optim\nimport torch.nn.functional as F\nfrom torch.utils.data import Dataset\nfrom torch.utils.data import DataLoader \n\nfrom tqdm import tqdm\nimport heapq\nimport csv\n\nimport numpy as np\nimport random\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nfrom matplotlib.font_manager import FontProperties\n\n\n#specify max length of sequence\nhindi_embedding_size = 29\nenglish_embedding_size = 32\n\nimport wandb\n# Instantiates the device to be used as GPU/CPU based on availability\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\ndevice.type","metadata":{"execution":{"iopub.status.busy":"2024-05-16T12:46:07.374415Z","iopub.execute_input":"2024-05-16T12:46:07.375307Z","iopub.status.idle":"2024-05-16T12:46:07.387222Z","shell.execute_reply.started":"2024-05-16T12:46:07.375276Z","shell.execute_reply":"2024-05-16T12:46:07.386166Z"},"trusted":true},"execution_count":6,"outputs":[{"execution_count":6,"output_type":"execute_result","data":{"text/plain":"'cuda'"},"metadata":{}}]},{"cell_type":"code","source":"#ANOTHER STYLE BEGIN\n\nimport numpy as np\n\n# Load Data to capture all characters\narray = np.loadtxt(\"/kaggle/input/aksharantar-sampled2/aksharantar_sampled/hin/hin_train.csv\",\n                 delimiter=\",\", dtype=str)\nnum_sample = array.shape[0]\nx_train, y_train = array[:, 0], array[:, 1]\n\nenglish_dict = {}\nhindi_dict = {}\nenglish_index_dict = {}\nhindi_index_dict = {}\n\n'''\nenglish_index = 3\nhin_index = 3'''\n\nenglish_index = hin_index = 3\n\nfor sentence in np.concatenate((x_train, y_train)):\n    for char in sentence:\n        if char not in english_dict:\n            english_dict[char] = english_index\n            english_index_dict[english_index] = char\n            english_index += 1\n\nfor sentence in y_train:\n    for char in sentence:\n        if char not in hindi_dict:\n            hindi_dict[char] = hin_index\n            hindi_index_dict[hin_index] = char\n            hin_index += 1\n\n# Adding start, stop and padding symbols\nstart_symbol = '<S>'\nend_symbol = '<E>'\npadding_symbol = '<P>'\nenglish_index_dict[0] = hindi_index_dict[0] = padding_symbol\nenglish_index_dict[1] = hindi_index_dict[1] = start_symbol\nenglish_index_dict[2] = hindi_index_dict[2] = end_symbol  #ANOTHER STYLE END\n","metadata":{"execution":{"iopub.status.busy":"2024-05-16T12:46:11.389259Z","iopub.execute_input":"2024-05-16T12:46:11.390181Z","iopub.status.idle":"2024-05-16T12:46:11.746073Z","shell.execute_reply.started":"2024-05-16T12:46:11.390141Z","shell.execute_reply":"2024-05-16T12:46:11.745251Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"code","source":"#ANOTHER STYLE BEGIN\n\nimport numpy as np\nimport torch\nfrom torch.utils.data import Dataset, DataLoader\n\nclass DataProcessor:\n    def __init__(self, english_embedding_size, hindi_embedding_size):\n        self.eng_embedd_size = english_embedding_size\n        self.hin_embedd_size = hindi_embedding_size\n        self.english_dict = {}\n        self.hindi_dict = {}\n\n    def process_data(self, path):\n        array = np.loadtxt(path, delimiter=\",\", dtype=str)\n        num_samples = array.shape[0]\n        x, y = array[:, 0], array[:, 1]\n\n        X = np.zeros((num_samples, self.english_embedding_size))  # input\n        Y = np.zeros((num_samples, self.hindi_embedding_size))  # target\n\n        for i in range(num_samples):\n            X[i][0] = Y[i][0] = 1\n\n            for j, char in enumerate(x[i]):\n                X[i][j + 1] = self.english_dict.setdefault(char, len(self.english_dict) + 3)\n\n            X[i][len(x[i]) + 1] = 2\n\n            for j, char in enumerate(y[i]):\n                Y[i][j + 1] = self.hindi_dict.setdefault(char, len(self.hindi_dict) + 3)\n\n            Y[i][len(y[i]) + 1] = 2\n\n        return X, Y\n\nclass CustomDataset(Dataset):\n    def __init__(self, X, Y):\n        self.X = torch.tensor(X, dtype=torch.int64)\n        self.Y = torch.tensor(Y, dtype=torch.int64)\n        self.length = X.shape[0]\n\n    def __getitem__(self, index):\n        return self.X[index], self.Y[index]\n\n    def __len__(self):\n        return self.length\n\ndata_processor = DataProcessor(english_embedding_size, hindi_embedding_size)\n\nX_train, y_train = data_processor.process_data(\"/kaggle/input/aksharantar-sampled2/aksharantar_sampled/hin/hin_train.csv\")\nX_val, y_val = data_processor.process_data(\"/kaggle/input/aksharantar-sampled2/aksharantar_sampled/hin/hin_valid.csv\")\nX_test, y_test = data_processor.process_data(\"/kaggle/input/aksharantar-sampled2/aksharantar_sampled/hin/hin_test.csv\")\n\ntrain_dataset = CustomDataset(X_train, y_train)\nval_dataset = CustomDataset(X_val, y_val)\ntest_dataset = CustomDataset(X_test, y_test)\n\ntrain_loader = DataLoader(train_dataset, shuffle=True, batch_size=256)\nval_loader = DataLoader(val_dataset, shuffle=True, batch_size=256)\ntest_loader = DataLoader(test_dataset, shuffle=False, batch_size=256)","metadata":{"execution":{"iopub.status.busy":"2024-05-16T12:46:14.982855Z","iopub.execute_input":"2024-05-16T12:46:14.983789Z","iopub.status.idle":"2024-05-16T12:46:16.232807Z","shell.execute_reply.started":"2024-05-16T12:46:14.983745Z","shell.execute_reply":"2024-05-16T12:46:16.231874Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"code","source":"# Importing necessary libraries\nimport torch\nimport torch.nn as nn\n\n# Defining the Encoder class as a subclass of nn.Module\nclass Encoder(nn.Module):\n    \n    # Initializing the Encoder class with default and custom parameters\n    def __init__(self,InputDimension=72,EmbeddingDimension=64,HiddenDimension=256,CellType='gru',layers=2,bi_directional=True,DropOut=0,\n                 device=torch.device('cuda' if torch.cuda.is_available() else 'cpu')):\n        super(Encoder, self).__init__()\n        \n        # Dictionary to store detailed parameters\n        self.detail_parameters = {}\n        self.detail_parameters['InputDimension'] = InputDimension\n        self.detail_parameters['EmbeddingDimension'] = EmbeddingDimension\n        self.detail_parameters['HiddenDimension'] = HiddenDimension\n        self.detail_parameters['CellType'] = CellType\n        self.detail_parameters['DropOut'] = DropOut\n        self.detail_parameters['layers'] = layers\n        self.detail_parameters['direction_value'] = 2 if bi_directional else 1\n        self.detail_parameters['device'] = device.type\n\n        # Assigning parameters to instance variables\n        self.InputDimension = InputDimension\n        self.EmbeddingDimension = EmbeddingDimension\n        self.HiddenDimension = HiddenDimension\n        self.CellType = CellType\n        self.layers = layers\n        self.dropout = DropOut\n        self.device = device\n\n        # Initializing Embedding layer\n        self.embedding = nn.Embedding(self.InputDimension, self.EmbeddingDimension)\n        self.dropout_layer = nn.Dropout(DropOut)\n        \n        # Calculating the direction value based on bidirectionality\n        self.direction_value = 2 if bi_directional else 1\n\n        # Defining different types of recurrent cells based on cell type\n        if self.cell_type == 'rnn':\n            self.encoder_type = RNNLayer(self.EmbeddingDimension, self.HiddenDimension, self.layers, bi_directional, DropOut)\n        elif self.cell_type == 'gru':\n            self.encoder_type = GRULayer(self.EmbeddingDimension, self.HiddenDimension, self.layers, bi_directional, DropOut)\n        elif self.cell_type == 'lstm':\n            self.encoder_type = LSTMLayer(self.EmbeddingDimension, self.HiddenDimension, self.layers, bi_directional, DropOut)\n\n    # Forward method for Encoder\n    def forward(self, input, hidden, cell=None):\n        embedded = self.embedding(input)\n        embedded = self.dropout_layer(embedded)\n        \n        # Handling LSTM separately for its cell state\n        if self.cell_type == 'lstm':\n            output, (hidden, cell) = self.encoder_type(embedded, (hidden, cell))\n        else:\n            output, hidden = self.encoder_type(embedded, hidden)\n\n        return output, hidden, cell if self.cell_type == 'lstm' else None\n\n    # Method to get detailed parameters\n    def getParams(self):\n        return self.detail_parameters\n    \n    # Method to initialize hidden state\n    def init_hidden(self, batch):\n        return torch.zeros(self.direction_value * self.layers, batch, self.HiddenDimension, device=self.device)\n\n# Define RNN layer as a subclass of nn.Module\nclass RNNLayer(nn.Module):\n    def __init__(self, input_size, hidden_size, num_layers, bidirectional, dropout):\n        super(RNNLayer, self).__init__()\n        self.rnn = nn.RNN(input_size=input_size, hidden_size=hidden_size, num_layers=num_layers, bidirectional=bidirectional, dropout=dropout)\n\n    def forward(self, input, hidden):\n        return self.rnn(input, hidden)\n\n# Define GRU layer as a subclass of nn.Module\nclass GRULayer(nn.Module):\n    def __init__(self, input_size, hidden_size, num_layers, bidirectional, dropout):\n        super(GRULayer, self).__init__()\n        self.gru = nn.GRU(input_size=input_size, hidden_size=hidden_size, num_layers=num_layers, bidirectional=bidirectional, dropout=dropout)\n\n    def forward(self, input, hidden):\n        return self.gru(input, hidden)\n\n# Define LSTM layer as a subclass of nn.Module\nclass LSTMLayer(nn.Module):\n    def __init__(self, input_size, hidden_size, num_layers, bidirectional, dropout):\n        super(LSTMLayer, self).__init__()\n        self.lstm = nn.LSTM(input_size=input_size, hidden_size=hidden_size, num_layers=num_layers, bidirectional=bidirectional, dropout=dropout)\n\n    def forward(self, input, hidden):\n        return self.lstm(input, hidden)  #ANOTHER STYLE END\n    \n    \n    \n    \nimport torch\nimport torch.nn as nn\n\nclass MyEncoder(nn.Module):\n    def __init__(self, input_size=72, embedding_size=64, hidden_size=256, cell_type='gru', num_layers=2, bidirectional=True, dropout_prob=0,\n                 device=torch.device('cuda' if torch.cuda.is_available() else 'cpu')):\n        super(MyEncoder, self).__init__()\n\n        # Configuration parameters for the encoder\n        self.config = {\n            'input_size': input_size,\n            'embedding_size': embedding_size,\n            'hidden_size': hidden_size,\n            'cell_type': cell_type,\n            'num_layers': num_layers,\n            'bidirectional': bidirectional,\n            'dropout_prob': dropout_prob,\n            'device': device.type\n        }\n\n        # Assigning parameters to instance variables\n        self.input_size = input_size\n        self.embedding_size = embedding_size\n        self.hidden_size = hidden_size\n        self.cell_type = cell_type\n        self.num_layers = num_layers\n        self.bidirectional = bidirectional\n        self.dropout_prob = dropout_prob\n        self.device = device\n\n        # Embedding layer\n        self.embedding_layer = nn.Embedding(input_size, embedding_size)\n        # Dropout layer\n        self.dropout_layer = nn.Dropout(dropout_prob)\n\n        # Calculating the number of directions based on bidirectionality\n        self.directions = 2 if bidirectional else 1\n\n        # Instantiating the appropriate RNN layer based on cell type\n        if cell_type == 'rnn':\n            self.rnn_layer = MyRNNLayer(embedding_size, hidden_size, num_layers, bidirectional, dropout_prob)\n        elif cell_type == 'gru':\n            self.rnn_layer = MyGRULayer(embedding_size, hidden_size, num_layers, bidirectional, dropout_prob)\n        elif cell_type == 'lstm':\n            self.rnn_layer = MyLSTMLayer(embedding_size, hidden_size, num_layers, bidirectional, dropout_prob)\n\n    def forward(self, input_seq, hidden_state, cell_state=None):\n        # Embedding the input sequence\n        embedded_seq = self.embedding_layer(input_seq)\n        # Applying dropout to the embedded sequence\n        embedded_seq = self.dropout_layer(embedded_seq)\n\n        # Forward pass through the RNN layer\n        if self.cell_type == 'lstm':\n            output_seq, (hidden_state, cell_state) = self.rnn_layer(embedded_seq, (hidden_state, cell_state))\n        else:\n            output_seq, hidden_state = self.rnn_layer(embedded_seq, hidden_state)\n\n        # Returning the output sequence and hidden/cell states (if LSTM)\n        return output_seq, hidden_state, cell_state if self.cell_type == 'lstm' else None\n\n    def get_config(self):\n        # Method to retrieve the configuration parameters\n        return self.config\n    \n    def init_hidden_state(self, batch_size):\n        # Method to initialize the hidden state\n        return torch.zeros(self.directions * self.num_layers, batch_size, self.hidden_size, device=self.device)\n\nclass MyRNNLayer(nn.Module):\n    def __init__(self, input_size, hidden_size, num_layers, bidirectional, dropout_prob):\n        super(MyRNNLayer, self).__init__()\n        # RNN layer instantiation\n        self.rnn = nn.RNN(input_size=input_size, hidden_size=hidden_size, num_layers=num_layers, bidirectional=bidirectional, dropout=dropout_prob)\n\n    def forward(self, input_seq, hidden_state):\n        # Forward pass through the RNN layer\n        return self.rnn(input_seq, hidden_state)\n\nclass MyGRULayer(nn.Module):\n    def __init__(self, input_size, hidden_size, num_layers, bidirectional, dropout_prob):\n        super(MyGRULayer, self).__init__()\n        # GRU layer instantiation\n        self.gru = nn.GRU(input_size=input_size, hidden_size=hidden_size, num_layers=num_layers, bidirectional=bidirectional, dropout=dropout_prob)\n\n    def forward(self, input_seq, hidden_state):\n        # Forward pass through the GRU layer\n        return self.gru(input_seq, hidden_state)\n\nclass MyLSTMLayer(nn.Module):\n    def __init__(self, input_size, hidden_size, num_layers, bidirectional, dropout_prob):\n        super(MyLSTMLayer, self).__init__()\n        # LSTM layer instantiation\n        self.lstm = nn.LSTM(input_size=input_size, hidden_size=hidden_size, num_layers=num_layers, bidirectional=bidirectional, dropout=dropout_prob)\n\n    def forward(self, input_seq, hidden_state):\n        # Forward pass through the LSTM layer\n        return self.lstm(input_seq, hidden_state)\n\n","metadata":{"execution":{"iopub.status.busy":"2024-05-16T12:46:20.299982Z","iopub.execute_input":"2024-05-16T12:46:20.300659Z","iopub.status.idle":"2024-05-16T12:46:20.319313Z","shell.execute_reply.started":"2024-05-16T12:46:20.300631Z","shell.execute_reply":"2024-05-16T12:46:20.318493Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nclass Decoder(nn.Module):\n    def __init__(self,InputDimension=26,EmbeddingDimension=64,HiddenDimension=256,CellType='lstm',layers=2,use_attention=False,\n                 attention_dimension=None,DropOut=0,bi_directional=True,\n                 device=torch.device('cuda' if torch.cuda.is_available() else 'cpu')):\n        super(Decoder, self).__init__()\n\n        self.InputDimension = InputDimension\n        self.EmbeddingDimension = EmbeddingDimension\n        self.HiddenDimension = HiddenDimension\n        self.CellType = CellType\n        self.layers = layers\n        self.use_attention = use_attention\n        self.attention_dimension = attention_dimension\n        self.DropOut = DropOut\n        self.device = device\n        #self.linear_transform = nn.Linear(hidden_dimension, output_dimension)  # Adjust output_dimension as needed\n\n        # Embedding layer\n        self.embedding = nn.Embedding(InputDimension, EmbeddingDimension)\n        self.dropout_layer = nn.Dropout(DropOut)\n\n        # Calculate input size considering attention\n        self.input_size = EmbeddingDimension\n        if use_attention:\n            self.input_size += attention_dimension\n\n        # Define decoder type (RNN, GRU, LSTM)\n        if CellType == 'rnn':\n            self.decoder_type = nn.RNN(input_size=self.input_size, hidden_size=HiddenDimension,\n                                        num_layers=layers, bi_directional=bi_directional, DropOut=DropOut)\n        elif CellType == 'gru':\n            self.decoder_type = nn.GRU(input_size=self.input_size, hidden_size=HiddenDimension,\n                                        num_layers=layers, bi_directional=bi_directional, DropOut=DropOut)\n        elif CellType == 'lstm':\n            self.decoder_type = nn.LSTM(input_size=self.input_size, hidden_size=HiddenDimension,\n                                         num_layers=layers, bi_directional=bi_directional, DropOut=DropOut)\n\n        # Attention mechanism components\n        if use_attention:\n            self.U = nn.Linear(HiddenDimension, HiddenDimension)\n            self.W = nn.Linear(HiddenDimension, HiddenDimension)\n            self.V = nn.Linear(HiddenDimension, 1)\n\n        # Output layer to match input dimension\n        self.W1 = nn.Linear(HiddenDimension * (2 if bi_directional else 1), InputDimension)\n    \n    def forward(self, input, hidden, cell=None, encoder_outputs=None):\n        embedded = self.embedding(input)\n        embedded = self.dropout_layer(embedded)\n\n        # Apply attention mechanism if enabled\n        if self.use_attention:\n            context, attention_weights = self.apply_attention(hidden, encoder_outputs)\n            embedded = torch.cat((embedded, context), 2)\n\n        # Pass through decoder RNN type\n        if self.CellType == 'lstm':\n            output, (hidden, cell) = self.decoder_type(embedded, (hidden, cell))\n        else:\n            output, hidden = self.decoder_type(embedded, hidden)\n\n        # Apply linear layer to match output dimension\n        output = self.W1(output)\n\n        return output, hidden, cell, attention_weights if self.use_attention else None\n    \n\n    \n    def apply_attention(self, hidden, encoder_outputs):\n    # Project encoder outputs and hidden state\n        encoder_transform = self.W(encoder_outputs)\n        hidden_transform = self.U(hidden)\n\n    # Combine encoder and hidden transformations\n        concat_transform = encoder_transform + hidden_transform\n\n    # Apply activation function\n        concat_transform = torch.tanh(concat_transform)\n\n    # Calculate attention scores\n        score = self.V(concat_transform)\n\n    # Apply softmax to get attention weights\n        attention_weights = F.softmax(score, dim=1)\n\n    # Compute context vector\n        context_vector = torch.sum(attention_weights * encoder_outputs, dim=1)\n\n    # Reshape context vector\n        normalized_context_vector = context_vector.unsqueeze(0)\n\n        return normalized_context_vector, attention_weights\n\n    \n    def getParams(self):\n        return {\n            'InputDimension': self.InputDimension,\n            'EmbeddingDimension': self.EmbeddingDimension,\n            'HiddenDimension': self.HiddenDimension,\n            'attention_dimension': self.attention_dimension,\n            'CellType': self.CellType,\n            'layers': self.layers,\n            'device': self.device.type,\n            'DropOut': self.DropOut,\n            'use_attention': self.use_attention,\n            'attention_dimension': self.attention_dimension\n        }\n    \n    \nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nclass MyDecoder(nn.Module):\n    def __init__(self, input_size=26, embedding_size=64, hidden_size=256, cell_type='lstm', num_layers=2, use_attention=False,\n                 attention_size=None, dropout=0, bidirectional=True,\n                 device=torch.device('cuda' if torch.cuda.is_available() else 'cpu')):\n        super(MyDecoder, self).__init__()\n\n        # Decoder parameters\n        self.input_size = input_size  # Number of unique input symbols\n        self.embedding_size = embedding_size  # Dimensionality of the embedding space\n        self.hidden_size = hidden_size  # Dimensionality of the hidden state\n        self.cell_type = cell_type  # Type of recurrent cell (RNN, GRU, LSTM)\n        self.num_layers = num_layers  # Number of recurrent layers\n        self.use_attention = use_attention  # Flag indicating whether to use attention mechanism\n        self.attention_size = attention_size  # Dimensionality of attention mechanism\n        self.dropout = dropout  # Dropout probability\n        self.bidirectional = bidirectional  # Flag indicating bidirectional RNN\n        self.device = device  # Device to run computations\n\n        # Components initialization\n        self._build_embedding_layer()  # Initialize embedding layer\n        self._build_dropout_layer()  # Initialize dropout layer\n        self._build_decoder_rnn()  # Initialize decoder RNN\n        if use_attention:\n            self._build_attention_mechanism()  # Initialize attention mechanism components\n        self._build_output_layer()  # Initialize output layer\n\n    def forward(self, input_seq, hidden_state, cell_state=None, encoder_outputs=None):\n        # Embed input sequence\n        embedded_seq = self.embedding(input_seq)\n        embedded_seq = self.dropout(embedded_seq)\n\n        # Apply attention mechanism if enabled\n        if self.use_attention:\n            context, attention_weights = self._apply_attention(hidden_state, encoder_outputs)\n            embedded_seq = torch.cat((embedded_seq, context), 2)\n\n        # Pass through decoder RNN\n        output, hidden_state, cell_state = self.decoder_rnn(embedded_seq, (hidden_state, cell_state) if self.cell_type == 'lstm' else hidden_state)\n\n        # Apply output layer to match output dimension\n        output = self.output_layer(output)\n\n        return output, hidden_state, cell_state, attention_weights if self.use_attention else None\n\n    # Method to build embedding layer\n    def _build_embedding_layer(self):\n        self.embedding = nn.Embedding(self.input_size, self.embedding_size)\n\n    # Method to build dropout layer\n    def _build_dropout_layer(self):\n        self.dropout = nn.Dropout(self.dropout)\n\n    # Method to build decoder RNN\n    def _build_decoder_rnn(self):\n        input_size = self.embedding_size + (self.attention_size if self.use_attention else 0)\n        if self.cell_type == 'lstm':\n            self.decoder_rnn = nn.LSTM(input_size=input_size, hidden_size=self.hidden_size, num_layers=self.num_layers,\n                                       bidirectional=self.bidirectional, dropout=self.dropout)\n        elif self.cell_type == 'gru':\n            self.decoder_rnn = nn.GRU(input_size=input_size, hidden_size=self.hidden_size, num_layers=self.num_layers,\n                                      bidirectional=self.bidirectional, dropout=self.dropout)\n        else:\n            self.decoder_rnn = nn.RNN(input_size=input_size, hidden_size=self.hidden_size, num_layers=self.num_layers,\n                                      bidirectional=self.bidirectional, dropout=self.dropout)\n\n    # Method to build attention mechanism\n    def _build_attention_mechanism(self):\n        self.attention_W = nn.Linear(self.hidden_size, self.hidden_size)\n        self.attention_U = nn.Linear(self.hidden_size, self.hidden_size)\n        self.attention_V = nn.Linear(self.hidden_size, 1)\n\n    # Method to build output layer\n    def _build_output_layer(self):\n        output_size = self.input_size\n        if self.bidirectional:\n            output_size *= 2\n        self.output_layer = nn.Linear(self.hidden_size, output_size)\n\n    # Method to apply attention mechanism\n    def _apply_attention(self, hidden_state, encoder_outputs):\n        encoder_transform = self.attention_W(encoder_outputs)\n        hidden_transform = self.attention_U(hidden_state)\n        concat_transform = encoder_transform + hidden_transform\n        concat_transform = torch.tanh(concat_transform)\n        score = self.attention_V(concat_transform)\n        attention_weights = F.softmax(score, dim=1)\n        context_vector = torch.sum(attention_weights * encoder_outputs, dim=1)\n        normalized_context_vector = context_vector.unsqueeze(0)\n\n        return normalized_context_vector, attention_weights\n\n    # Method to get decoder parameters\n    def get_params(self):\n        return {\n            'input_size': self.input_size,\n            'embedding_size': self.embedding_size,\n            'hidden_size': self.hidden_size,\n            'attention_size': self.attention_size,\n            'cell_type': self.cell_type,\n            'num_layers': self.num_layers,\n            'dropout': self.dropout.p,\n            'bidirectional': self.bidirectional,\n            'device': self.device.type,\n            'use_attention': self.use_attention,\n            'attention_size': self.attention_size\n        }\n","metadata":{"execution":{"iopub.status.busy":"2024-05-16T12:46:23.718430Z","iopub.execute_input":"2024-05-16T12:46:23.719318Z","iopub.status.idle":"2024-05-16T12:46:23.737952Z","shell.execute_reply.started":"2024-05-16T12:46:23.719284Z","shell.execute_reply":"2024-05-16T12:46:23.736927Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"code","source":"import torch\nimport heapq\n\nclass BeamNode:\n    def __init__(self, index, path_probability, hidden_state, cell_state, parent=None):\n        self.index = index\n        self.path_probability = path_probability\n        self.hidden_state = hidden_state\n        self.cell_state = cell_state\n        self.parent = parent\n        self.length = 0\n\ndef expand_node(model, node):\n    output, dec_hidden, cell, _ = model.decoder.forward(node.index, node.hidden, node.cell, None)\n    output = model.softmax(output, dim=2)\n    topk_output, topk_index = torch.topk(output, model.beam_width, dim=2)\n    return topk_output, topk_index, dec_hidden, cell\n\ndef create_child_nodes(model, topk_output, topk_index, dec_hidden, cell, curr_node):\n    child_nodes = []\n    for j in range(model.beam_width):\n        output = topk_output[:, :, j]\n        index = topk_index[:, :, j]\n        if curr_node.path_probability * output.item() < 0.001:\n            continue\n        child_node = BeamNode(output.item(), curr_node.path_probability * output.item(), index, dec_hidden, cell, curr_node)\n        child_node.length = curr_node.length + 1\n        child_nodes.append(child_node)\n    return child_nodes\n\ndef traverse_path(model, path, predicted):\n    while path is not None:\n        output, _, _, _ = model.decoder.forward(path.index, path.hidden, path.cell, None)\n        predicted[model.output_seq_length - path.length, i:i+1] = output\n        path = path.parent\n\ndef beam_search(model, outputs, dec_hiddens, cells, predicted):\n    batch_size = outputs.shape[1]\n    paths = []\n\n    for i in range(batch_size):\n        with torch.no_grad():\n            model.eval()\n            output = outputs[:, i:i+1].contiguous()\n            index = output.contiguous()\n            dec_hidden = dec_hiddens[:, i:i+1, :].contiguous()\n            cell = cells[:, i:i+1, :].contiguous() if cells is not None else None\n            \n            open_list = []\n            heapq.heapify(open_list)\n            \n            root_node = BeamNode(1, 1, index, dec_hidden, cell, None)\n            heapq.heappush(open_list, root_node)\n\n            while len(open_list) > 0:\n                curr_node = heapq.heappop(open_list)\n                \n                if curr_node.length == model.output_seq_length - 1:\n                    paths.append(curr_node)\n                    continue\n\n                topk_output, topk_index, dec_hidden, cell = expand_node(model, curr_node)\n                child_nodes = create_child_nodes(model, topk_output, topk_index, dec_hidden, cell, curr_node)\n                for node in child_nodes:\n                    heapq.heappush(open_list, node)\n\n            if len(paths) > 0:\n                best_path = min(paths, key=lambda x: x.path_probability)\n                traverse_path(model, best_path, predicted)\n            else:\n                for t in range(1, model.output_seq_length):\n                    output, _, _, _ = model.decoder.forward(index, dec_hidden, cell, None)\n                    predicted[t, i:i+1] = output\n                    output = model.softmax(output, dim=2)\n                    output = torch.argmax(output, dim=2)","metadata":{"execution":{"iopub.status.busy":"2024-05-16T12:47:36.138120Z","iopub.execute_input":"2024-05-16T12:47:36.138473Z","iopub.status.idle":"2024-05-16T12:47:36.156700Z","shell.execute_reply.started":"2024-05-16T12:47:36.138435Z","shell.execute_reply":"2024-05-16T12:47:36.155655Z"},"trusted":true},"execution_count":11,"outputs":[]},{"cell_type":"code","source":"def scoring(y_dash , y):\n    num_sample,seq_len = y.shape\n    score = torch.sum(torch.sum(y_dash == y,axis = 1) == seq_len)\n    return score\n\ndef scoring_prx(y_dash, y):\n    if y_dash.dim() == 1:\n        y_dash = y_dash.unsqueeze(0)  # Add a batch dimension\n    elif y_dash.dim() == 2 and y_dash.size(1) != y.size(1):\n        y_dash = y_dash.view(y.size(0), -1)  # Reshape y_dash to match the shape of y\n    correct = torch.sum(torch.all(torch.eq(y_dash, y), dim=1))\n    return correct","metadata":{"execution":{"iopub.status.busy":"2024-05-16T12:50:20.166830Z","iopub.execute_input":"2024-05-16T12:50:20.167465Z","iopub.status.idle":"2024-05-16T12:50:20.174384Z","shell.execute_reply.started":"2024-05-16T12:50:20.167415Z","shell.execute_reply":"2024-05-16T12:50:20.173502Z"},"trusted":true},"execution_count":13,"outputs":[]},{"cell_type":"code","source":"class Seq2Seq(nn.Module):\n    \n   \n    #This class incorporate the whole transliteration model. It calls encoder and pass output of encoder\n    #to decoder with or wihout attention. Parameters are specified in constructor.\n    \n    \n    def __init__(self, input_seq_length = 32,output_seq_length = 29,encoder_input_dimension = 29, decoder_input_dimension = 72,encoder_hidden_dimension = 256, \n                 decoder_hidden_dimension =256,encoder_embed_dimension = 256, decoder_embed_dimension = 256, bidirectional = True,encoder_num_layers = 3,\n                 decoder_num_layers = 2,cell_type = 'lstm', dropout = 0.2,beam_width = 3,device = device,attention = False):\n        \n        \n        super(Seq2Seq, self).__init__()\n        \n        self.detail_parameters = {}\n        self.detail_parameters['input_seq_length'] = input_seq_length\n        self.detail_parameters['output_seq_length'] = output_seq_length\n        self.detail_parameters['encoder_input_dimension'] = encoder_input_dimension\n        self.detail_parameters['decoder_input_dimension'] = decoder_input_dimension\n        self.detail_parameters['encoder_hidden_dimension'] = encoder_hidden_dimension\n        self.detail_parameters['encoder_embed_dimension'] = encoder_embed_dimension\n        self.detail_parameters['decoder_hidden_dimension'] = decoder_hidden_dimension\n        self.detail_parameters['decoder_embed_dimension'] = decoder_embed_dimension\n        self.detail_parameters['bidirectional'] = bidirectional\n        self.detail_parameters['encoder_num_layers'] = encoder_num_layers\n        self.detail_parameters['decoder_num_layers'] = decoder_num_layers\n        self.detail_parameters['cell_type'] = cell_type\n        self.detail_parameters['dropout'] = dropout\n        self.detail_parameters['device'] = device.type\n\n        \n        \n        # Input sequence length => max_length of english\n        self.input_seq_length = input_seq_length\n        \n        # Output sequence length => max_length of malayalam\n        self.output_seq_length = output_seq_length\n        \n        # total number of english characters\n        self.encoder_input_dimension = encoder_input_dimension\n        \n        # total number of malayalam characters\n        self.decoder_input_dimension = decoder_input_dimension\n        \n        # Hidden dim for encoder\n        self.encoder_hidden_dimension = encoder_hidden_dimension\n        \n        # Hidden dim for decoder\n        self.decoder_hidden_dimension = decoder_hidden_dimension\n        \n        # Dimension to which we need to embed our source input\n        self.encoder_embed_dimension = encoder_embed_dimension\n        \n        # Dimension to which we need to embed our target input\n        self.decoder_embed_dimension = decoder_embed_dimension\n        \n        # Whether bidirection needed or not and sets its value as 2, so as to multiply hidden by 2\n        self.direction = bidirectional\n        self.direction_value = 2 if bidirectional else 1\n        \n        # Number of layers for encoder and decoder\n        self.encoder_num_layers = encoder_num_layers\n        self.decoder_num_layers = decoder_num_layers\n        \n        # Which cell type to use\n        self.cell_type = cell_type \n        \n        # Whether to use dropout or not\n        self.dropout = dropout\n        self.device = device\n        \n        self.softmax = F.softmax\n        \n        # fix beam width\n        self.beam_width = beam_width\n        \n        # Whether to use attention or not \n        self.use_attention = attention\n        \n        # Linear Weights so as to make encoder and decoder dimension same (i.e., if they differ by hidden dim or layer)\n        self.enc_dec_linear1 = nn.Linear(encoder_hidden_dimension,decoder_hidden_dimension)\n        self.enc_dec_linear2 = nn.Linear(encoder_num_layers*self.direction_value,decoder_num_layers*self.direction_value)\n        \n        # Linear Weights so as to make encoder and decoder cell's dimension same (i.e., if they differ by hidden dim or layer)\n        self.enc_dec_cell_linear1 = nn.Linear(encoder_hidden_dimension,decoder_hidden_dimension)\n        self.enc_dec_cell_linear2 = nn.Linear(encoder_num_layers*self.direction_value,decoder_num_layers*self.direction_value)\n        \n        # Linear Weights so as to make encoder and decoder attention dimension same (i.e., if they differ by hidden dim or layer)\n        self.enc_dec_att_linear1 = nn.Linear(encoder_hidden_dimension,decoder_hidden_dimension)\n        self.enc_dec_att_linear2 = nn.Linear(encoder_num_layers*self.direction_value,decoder_num_layers*self.direction_value)\n        \n        # initialize encoder\n        self.encoder = Encoder(input_dimension = self.encoder_input_dimension,embed_dimension = self.encoder_embed_dimension, \n                               hidden_dimension =  self.encoder_hidden_dimension,cell_type = self.cell_type,layers = self.encoder_num_layers,\n                               bidirectional = self.direction,dropout = self.dropout, device = self.device)\n        \n        # initialize decoder\n        self.decoder = Decoder(input_dimension = self.decoder_input_dimension,embed_dimension = self.decoder_embed_dimension,hidden_dimension = self.decoder_hidden_dimension,\n                               attention_dimension = self.decoder_hidden_dimension,cell_type = self.cell_type,layers = self.decoder_num_layers,\n                               dropout = self.dropout,device = self.device,use_attention = self.use_attention)\n        \n    def getParams(self):\n        return self.detail_parameters\n    \n    def forward(self, input, target ,teacher_force, acc_calculate = False):\n        \n        batch_size = input.shape[0]\n        \n        #initialize hidden dimension o pass to encoder\n        enc_hidden = self.encoder.init_hidden(batch_size)\n        \n        # if lstm then initialize cell also\n        if self.cell_type == 'lstm':\n            cell = self.encoder.init_hidden(batch_size)\n        else:\n            cell = None\n        \n        encoder_outputs = None\n        \n        # if using attention, then encoder outputs should be stored \n        if self.use_attention:\n            encoder_outputs = torch.zeros(self.input_seq_length,self.direction_value*self.decoder_num_layers,batch_size,self.decoder_hidden_dimension,device=device)\n        \n        # Pass input to encoder one by character in batch fashion\n        for t in range(self.input_seq_length):\n            enc_output,enc_hidden, cell = self.encoder.forward(input[:,t].unsqueeze(0), enc_hidden, cell)\n            \n            # Store encoder outputs, by first converting into same dimesnion by linear layers\n            if self.use_attention:\n                enc_hidden_new = enc_hidden\n                enc_hidden_new = self.enc_dec_att_linear1(enc_hidden_new)\n                enc_hidden_new = enc_hidden_new.permute(2,1,0).contiguous()\n                enc_hidden_new = self.enc_dec_att_linear2(enc_hidden_new)\n                enc_hidden_new = enc_hidden_new.permute(2,1,0).contiguous()\n                encoder_outputs[t] = enc_hidden_new\n        \n        # Encoder's last state is decoders first state\n        enc_last_state = enc_hidden\n        \n        # predicted to store all predictions by model to calculate loss\n        predicted = torch.zeros(self.output_seq_length, batch_size, self.decoder_input_dimension,device = self.device)\n        \n        # Store all attention weights, so can be used for plotting attn heatmaps\n        attn_weights = torch.zeros(self.output_seq_length, self.input_seq_length, self.direction_value*self.decoder_num_layers ,batch_size, device = self.device)\n        \n        # Encoders last state is decoders hidden also ransform in case they are of different dimension\n        dec_hidden = enc_last_state\n        dec_hidden = self.enc_dec_linear1(dec_hidden)\n\n        dec_hidden = dec_hidden.permute(2,1,0).contiguous()\n        dec_hidden = self.enc_dec_linear2(dec_hidden)\n        dec_hidden = dec_hidden.permute(2,1,0).contiguous()\n        \n        # Here also, encoders last cell is decoders first cell, also transform to same dimesnion\n        if  self.cell_type == 'lstm':\n            cell = self.enc_dec_cell_linear1(cell)\n            cell = cell.permute(2,1,0).contiguous()\n            cell = self.enc_dec_cell_linear2(cell)\n            cell = cell.permute(2,1,0).contiguous()\n            \n\n        # output at start is all 1's <SOS>\n        output = torch.ones(1,batch_size,dtype=torch.long, device=self.device)\n        predicted[0,:,1]=torch.ones(batch_size)\n        attention_weights = None\n        \n        \n        # Do decoding by char by char fashion by batch   \n        for t in range(1,self.output_seq_length):\n            # if teacher forcing, then pass target directly\n            if teacher_force:\n                output,dec_hidden,cell,attention_weights=self.decoder.forward(target[:,t-1].unsqueeze(0),dec_hidden,cell,encoder_outputs)\n                predicted[t] = output.squeeze(0)\n\n            else:\n                # if beam is to be used, call beam instead of passing output from decoder\n                if self.beam_width > 1 and acc_calculate:\n                    beam = BeamSearch()\n                    beam.beamSearch(self, output,dec_hidden,cell, predicted)\n                    break\n                    \n                # call decoder one at a time\n                output,dec_hidden,cell,attention_weights=self.decoder.forward(output,dec_hidden,cell,encoder_outputs)\n                #store output in prediced (it containes probabilities)\n                predicted[t] = output.squeeze(0)\n                if self.use_attention:\n                    attn_weights[t] = attention_weights.squeeze(3)\n                    \n                # Convert output such that, it can be easily given to input\n                output = self.softmax(output,dim=2)\n                output = torch.argmax(output,dim=2)\n\n        \n        return predicted,attn_weights","metadata":{"execution":{"iopub.status.busy":"2024-05-16T12:51:46.477349Z","iopub.execute_input":"2024-05-16T12:51:46.477730Z","iopub.status.idle":"2024-05-16T12:51:46.509108Z","shell.execute_reply.started":"2024-05-16T12:51:46.477702Z","shell.execute_reply":"2024-05-16T12:51:46.508137Z"},"trusted":true},"execution_count":14,"outputs":[]},{"cell_type":"code","source":"def train(data_loader, val_loader ,epochs, beam):\n        \n        # Set all training parameters\n        optimizer = optim.Adam(model.parameters())\n        criterion = nn.CrossEntropyLoss()\n        # set model to train mode\n        model.train()\n        attention_weights = None\n        \n        # Do training in epoch fashion\n        for epoch in tqdm(range(epochs)):\n            total_loss=0\n            train_loss = 0\n            train_score = 0\n            val_score = 0\n            val_loss = 0\n            \n            # use data loader and enumerate each of data for training in batchwise\n            for i, (source, target) in enumerate(data_loader):\n\n                source = source.to(device)\n                target = target.to(device)\n                \n                \n                \n                optimizer.zero_grad()\n                \n                output,attention_weights = model.forward(source, target, epoch < epochs/2, False)                \n\n                # In order to do loss calc, first need to convert target to one-hot and make predicted in probabilistic manner\n                output = output.permute(1, 0, 2)\n                expected = F.one_hot(target,num_classes = 72).float()\n                    \n                # make predicted and target in same dimension\n                output = output.reshape(-1, 72)\n                expected = expected.reshape(-1,72)\n\n                # Calculate loss\n                loss = criterion(output, expected)\n                \n                # Calculate gradients\n                loss.backward()\n                \n                # Clip gradiens, so will not explode\n                nn.utils.clip_grad_norm_(model.parameters(),1)\n                \n                #update parameters\n                optimizer.step()\n                \n #                 break\n#             continue\n\n            # Calculate validation accuracy and losses => Same process as training, but here no updation of gradients\n            with torch.no_grad():\n                model.eval()\n\n                for val_input, val_target in val_loader:\n                    val_input = val_input.to(device)\n                    val_target = val_target.to(device)\n                    #val_output,_ = model.forward(val_input, None, False ,False)\n                    val_output,_ = model.forward(val_input, val_target, False ,False)\n                    \n                    acc_output = F.softmax(val_output,dim=2)\n                    acc_output = torch.argmax(acc_output,dim=2)\n                    acc_output = acc_output.T\n                    val_score += scoring(acc_output,val_target)\n\n                    \n                    val_output = val_output.permute(1, 0, 2)\n                    expected = F.one_hot(val_target,num_classes = 72).float()\n\n                    val_output = val_output.reshape(-1, 72)\n\n                    expected = expected.reshape(-1,72)\n\n                    \n                    loss = criterion(val_output, expected)\n                    val_loss += loss.item()\n              \n            # Calculate training accuracy and losses\n            with torch.no_grad():\n                model.eval()\n                for train_input, train_target in data_loader:\n                    train_input = train_input.to(device)\n                    train_target = train_target.to(device)\n                    train_output,_ = model.forward(train_input, None,False)\n                    \n                    acc_output = F.softmax(train_output,dim=2)\n                    acc_output = torch.argmax(acc_output,dim=2)\n                    acc_output = acc_output.T\n                    train_score += scoring(acc_output,train_target)\n\n                    \n                    train_output = train_output.permute(1, 0, 2)\n                    expected = F.one_hot(train_target,num_classes = 72).float()\n\n                    train_output = train_output.reshape(-1, 72)\n\n                    expected = expected.reshape(-1,72)\n\n                    \n                    loss = criterion(train_output, expected)\n                    train_loss += loss.item()\n                    \n                # Make the model trainable again\n                model.train()\n            \n                \n                \n            print(f'epoch {epoch}')\n            print(f'train loss => {train_loss/len(data_loader)} \\ntrain_acc => {train_score/len(data_loader.dataset)}')\n            print(f'valid loss => {val_loss/len(val_loader)} \\nvalid_acc => {val_score/len(val_loader.dataset)}')\n\n'''\ndef train(data_loader, val_loader, epochs, beam):\n    optimizer = optim.Adam(model.parameters())\n    criterion = nn.CrossEntropyLoss()\n    model.train()\n    attention_weights = None\n    \n    for epoch in tqdm(range(epochs)):\n        train_loss, train_score = _run_epoch(data_loader, optimizer, criterion, model, epoch, epochs, attention_weights, train_mode=True)\n        val_loss, val_score = _run_epoch(val_loader, None, criterion, model, epoch, epochs, attention_weights, train_mode=False)\n        \n        print(f'epoch {epoch}')\n        print(f'train loss => {train_loss} \\ntrain_acc => {train_score}')\n        print(f'valid loss => {val_loss} \\nvalid_acc => {val_score}')\n\n\ndef _run_epoch(data_loader, optimizer, criterion, model, epoch, epochs, attention_weights, train_mode=True):\n    total_loss = 0\n    total_score = 0\n    \n    model.train() if train_mode else model.eval()\n    \n    with torch.set_grad_enabled(train_mode):\n        for i, (source, target) in enumerate(data_loader):\n            source, target = source.to(device), target.to(device)\n            optimizer.zero_grad() if optimizer else None\n            output, attention_weights = model.forward(source, target, epoch < epochs/2, False)\n            output = output.permute(1, 0, 2)\n            expected = F.one_hot(target, num_classes=72).float()\n            output, expected = output.reshape(-1, 72), expected.reshape(-1, 72)\n            loss = criterion(output, expected)\n            loss.backward() if optimizer else None\n            nn.utils.clip_grad_norm_(model.parameters(), 1) if optimizer else None\n            optimizer.step() if optimizer else None\n            \n            total_loss += loss.item()\n            \n            if train_mode:\n                acc_output = F.softmax(output, dim=1)\n                acc_output = torch.argmax(acc_output, dim=1)\n                # Reshape acc_output to match the shape of target\n                acc_output = acc_output.view(target.size())\n                total_score += scoring(acc_output, target)\n    \n    avg_loss = total_loss / len(data_loader)\n    avg_score = total_score / len(data_loader.dataset)\n    \n    return avg_loss, avg_score'''\n\n\nmodel = Seq2Seq(\n    encoder_hidden_dimension=256, \n    decoder_hidden_dimension=256,\n    encoder_embed_dimension=256, \n    decoder_embed_dimension=256, \n    bidirectional=True,\n    encoder_num_layers=3,\n    decoder_num_layers=2,\n    cell_type='lstm', \n    dropout=0.2,\n    beam_width=3,\n    device=device,\n    attention=False  # Change attention to use_attention\n)\n\nmodel.to(device)\nepochs = 10\ntrain(train_loader, val_loader, epochs, False)","metadata":{"execution":{"iopub.status.busy":"2024-05-16T12:52:35.462993Z","iopub.execute_input":"2024-05-16T12:52:35.463408Z","iopub.status.idle":"2024-05-16T12:57:22.307633Z","shell.execute_reply.started":"2024-05-16T12:52:35.463380Z","shell.execute_reply":"2024-05-16T12:57:22.306192Z"},"trusted":true},"execution_count":15,"outputs":[{"name":"stderr","text":" 10%|█         | 1/10 [00:36<05:28, 36.48s/it]","output_type":"stream"},{"name":"stdout","text":"epoch 0\ntrain loss => 1.5944575417041777 \ntrain_acc => 0.0\nvalid loss => 1.3929478898644447 \nvalid_acc => 0.0\n","output_type":"stream"},{"name":"stderr","text":" 20%|██        | 2/10 [01:12<04:47, 35.99s/it]","output_type":"stream"},{"name":"stdout","text":"epoch 1\ntrain loss => 1.3363674068450928 \ntrain_acc => 0.011171874590218067\nvalid loss => 1.0846822932362556 \nvalid_acc => 0.03125\n","output_type":"stream"},{"name":"stderr","text":" 30%|███       | 3/10 [01:47<04:10, 35.77s/it]","output_type":"stream"},{"name":"stdout","text":"epoch 2\ntrain loss => 1.182860615849495 \ntrain_acc => 0.1096484363079071\nvalid loss => 0.9436689093708992 \nvalid_acc => 0.155029296875\n","output_type":"stream"},{"name":"stderr","text":" 40%|████      | 4/10 [02:23<03:34, 35.68s/it]","output_type":"stream"},{"name":"stdout","text":"epoch 3\ntrain loss => 1.1087348762154579 \ntrain_acc => 0.18617187440395355\nvalid loss => 0.8968608416616917 \nvalid_acc => 0.226806640625\n","output_type":"stream"},{"name":"stderr","text":" 50%|█████     | 5/10 [02:58<02:58, 35.71s/it]","output_type":"stream"},{"name":"stdout","text":"epoch 4\ntrain loss => 1.0265201181173325 \ntrain_acc => 0.26054686307907104\nvalid loss => 0.868542242795229 \nvalid_acc => 0.2783203125\n","output_type":"stream"},{"name":"stderr","text":" 60%|██████    | 6/10 [03:34<02:22, 35.74s/it]","output_type":"stream"},{"name":"stdout","text":"epoch 5\ntrain loss => 0.5348665207624436 \ntrain_acc => 0.07634765654802322\nvalid loss => 0.47036867402493954 \nvalid_acc => 0.098876953125\n","output_type":"stream"},{"name":"stderr","text":" 70%|███████   | 7/10 [04:10<01:47, 35.76s/it]","output_type":"stream"},{"name":"stdout","text":"epoch 6\ntrain loss => 0.47107376873493195 \ntrain_acc => 0.15576171875\nvalid loss => 0.42788021452724934 \nvalid_acc => 0.179931640625\n","output_type":"stream"},{"name":"stderr","text":" 70%|███████   | 7/10 [04:43<02:01, 40.43s/it]\n","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","Cell \u001b[0;32mIn[15], line 181\u001b[0m\n\u001b[1;32m    179\u001b[0m model\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m    180\u001b[0m epochs \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m10\u001b[39m\n\u001b[0;32m--> 181\u001b[0m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mval_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n","Cell \u001b[0;32mIn[15], line 86\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(data_loader, val_loader, epochs, beam)\u001b[0m\n\u001b[1;32m     84\u001b[0m train_input \u001b[38;5;241m=\u001b[39m train_input\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m     85\u001b[0m train_target \u001b[38;5;241m=\u001b[39m train_target\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[0;32m---> 86\u001b[0m train_output,_ \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mforward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_input\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m     88\u001b[0m acc_output \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39msoftmax(train_output,dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m)\n\u001b[1;32m     89\u001b[0m acc_output \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39margmax(acc_output,dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m)\n","Cell \u001b[0;32mIn[14], line 155\u001b[0m, in \u001b[0;36mSeq2Seq.forward\u001b[0;34m(self, input, target, teacher_force, acc_calculate)\u001b[0m\n\u001b[1;32m    153\u001b[0m \u001b[38;5;66;03m# Pass input to encoder one by character in batch fashion\u001b[39;00m\n\u001b[1;32m    154\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m t \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minput_seq_length):\n\u001b[0;32m--> 155\u001b[0m     enc_output,enc_hidden, cell \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencoder\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mforward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[43m,\u001b[49m\u001b[43mt\u001b[49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43munsqueeze\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43menc_hidden\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcell\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    157\u001b[0m     \u001b[38;5;66;03m# Store encoder outputs, by first converting into same dimesnion by linear layers\u001b[39;00m\n\u001b[1;32m    158\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39muse_attention:\n","Cell \u001b[0;32mIn[9], line 56\u001b[0m, in \u001b[0;36mEncoder.forward\u001b[0;34m(self, input, hidden, cell)\u001b[0m\n\u001b[1;32m     53\u001b[0m embedded \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdropout_layer(embedded)\n\u001b[1;32m     55\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcell_type \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlstm\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[0;32m---> 56\u001b[0m     output, (hidden, cell) \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencoder_type\u001b[49m\u001b[43m(\u001b[49m\u001b[43membedded\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mhidden\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcell\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     57\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     58\u001b[0m     output, hidden \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mencoder_type(embedded, hidden)\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n","Cell \u001b[0;32mIn[9], line 93\u001b[0m, in \u001b[0;36mLSTMLayer.forward\u001b[0;34m(self, input, hidden)\u001b[0m\n\u001b[1;32m     92\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m, hidden):\n\u001b[0;32m---> 93\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlstm\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhidden\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/rnn.py:879\u001b[0m, in \u001b[0;36mLSTM.forward\u001b[0;34m(self, input, hx)\u001b[0m\n\u001b[1;32m    876\u001b[0m         hx \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpermute_hidden(hx, sorted_indices)\n\u001b[1;32m    878\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m batch_sizes \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 879\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[43m_VF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlstm\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_flat_weights\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnum_layers\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    880\u001b[0m \u001b[43m                      \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdropout\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtraining\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbidirectional\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbatch_first\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    881\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    882\u001b[0m     result \u001b[38;5;241m=\u001b[39m _VF\u001b[38;5;241m.\u001b[39mlstm(\u001b[38;5;28minput\u001b[39m, batch_sizes, hx, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_flat_weights, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbias,\n\u001b[1;32m    883\u001b[0m                       \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_layers, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdropout, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtraining, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbidirectional)\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "],"ename":"KeyboardInterrupt","evalue":"","output_type":"error"}]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}